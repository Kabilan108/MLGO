
<h2>Compression of Differential Expression Data with Deep Autoencoders</h2>
<h4>Tony Kabilan Okeke</h4>
<h5>BMES 547: Course Project</h5>


# Introduction

## Problem Description {.smaller}

- **Differential Expression Analysis** is used to identify genes that have
  different levels of expression between >2 samples/conditions.
- **Gene Ontology Enrichment** analyzes functional annotations of differentially
  expressed genes.
- Extracting biologically significant information from genomic data is a
  difficult task.
  - The high dimensionality and complexity of genomic data makes it hard to
    build accurate and interpretable prediction models for protein function
- What data?
  - Microarrays
  - RNA-seq

::: {.notes}
- GO Enrichment
  - GO terms are used to describe biological processes, cellular components, and
    molecular functions.
  - Use statistical methods to identify functional annotations that are
    over or under-represented in a set of DEGs
- RNA-seq & MA
  - Techniques for measuring gene expression
  - RNA-seq is more sensitive than MA
  - RNA-seq allows you to measure expression of whole transcriptomes instead of
    just known genes
:::

## Problem Description {.smaller}

![](images/goterms.png){fig-align="center"}

## Background: Autoencoders {.smaller}

::: {.columns}

::: {.column width=50%}
- Autoencoders are a type of neural network that are used to learn efficient
  data encodings in an unsupervised manner.
- Two components
  - Encoder: compress input into latent space representation
  - Decoder: reconstruct original input from latent space
- Trained to minimize reconstruction error
:::

::: {.column width=50%}
::: {.fragment}
![](images/ae.png){fig-align="center"}
:::
:::

::::

::: {.notes}
- Uses
  - Dimensionality reduction
  - Feature extraction
  - Data denoising
:::

## Goals {.smaller}

- Develop a deep autoencoder model to learn a compressed representation of
  differential expression data (i.e., log2 fold change values).
- Use the latent representation of the autoencoder to predict GO terms.
  - Identify genes responsible for GO term enrichment.
  - <p style="color: red">Couldn't complete due to time constraints.</p>


# The Datasets

## Digital Expression Explorer 2 {.smaller}

::: {.columns}

::: {.column width=60%}
- Repository of uniformly processed RNA-seq data mined from public data
  obtained from the [Sequence Read Archive](https://www.ncbi.nlm.nih.gov/sra).
- DEE2 provides 532711 processed RNA-seq runs for Mouse genes and 475547
  processed RNA-seq runs for Human genes.
:::

::: {.column width=40%}
::: {.r-stack}
![](images/dee_datasets.png){.fragment width="400" height="400" fig-align="center"}

![](images/dee2.png){.fragment width="400" height="400" fig-align="center"}
:::
:::

::::

## The Dataset {.smaller}

- Leveraged DEE2 to curate 11130 unique datasets with corresponding GEO series
  including over 350,000 individual sequencing runs (SRRs) from experiments
  on *Mus musculus*.
- Of these, 7130 datasets met the inclusion criteria for our analysis.
  - These datasets included 48,978 different gene transcripts.
- Differential expression anaysis was performed on each dataset and the
  resulting log2 fold change values were stored.
  - GO enrichment was then performed and enriched GO terms were stored.


# Methods

## Methods: Data Curation {.smaller}

::: {.columns}

::: {.column .nonincremental width=55%}
- Filtered out datasets with no GEO accession - no metadata
- Analyzed GEO metadata to select datasets that include >1 groups
- Differential Expression & GO enrichment
  - Kept GO terms with $p < 0.05$
- Combined datasets into a single feature matrix
- One-hot encoded GO terms
  - 11,130 unique GO terms
- Train/Test/Validation split
- Data standardized
:::

::: {.column width=45%}
```{.r code-line-numbers="|1-4|6-11"}
deg <- limma::lmFit(counts, design) %>%
  limma::contrasts.fit(contrast) %>%
  limma::eBayes() %>%
  limma::topTable(number = Inf)

go <- GOfuncR::go_enrich(
    deg[c("symbol", "is_candidate")],
    organismDb = "org.Mm.eg.db",
    n_randsets = 100,
    silent = TRUE
  )$results
```

<p style='opacity:0'>...</p>

```{.python code-line-numbers="|1"}
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)
```
:::

::::

::: {.notes}
- Used `doSNOW` to parallelize the data curation process
- Split tasks into different scripts
- Combined them in a bash script on Google Cloud (more RAM)
:::

## Methods: Dimension Reduction {.smaller}

::: {.columns}

::: {.column .nonincremental width=55%}
- `VarianceThreshold`
  - Several genes had very low variances across all datasets which impacted
    the performance of PCA.
- `PCA`
- `t-SNE`
  - Non-linear D.R. technique for high-dimensional data
  - Used in RNA-seq analysis to identify clusters of similar genes/cells
:::

::: {.column .nonincremental width=45%}
```{.python code-line-numbers="|1-2|7-8|10-11"}
sel = VarianceThreshold(threshold=1)
sel.fit(X)

X_train = X_train[:, sel.get_support()]
X_val = X_val[:, sel.get_support()]

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

tsne = TSNE(n_components=2, perplexity=50, n_jobs=mp.cpu_count())
X_tsne = tsne.fit_transform(X)
```
:::

::::

::: {.notes}
- t-SNE
  - finding a low-dimensional space where data points are arranged in a way
    that reflects underlying similarities.
:::

## Methods: Autoencoder {.smaller}

::: {.columns}

::: {.column .nonincremental width=50%}
- Implemented the NN using `Keras` with `TensorFlow` backend
  - Used `Adam` optimizer
  - `MSE` loss function
  - `ReLU` activation
- Trained for 50 epochs
  - Evaluated `validation loss`
- `ReduceLROnPlateau` callback
  - Reduce learning rate when `val_loss` stops improving
:::

::: {.column .nonincremental width=50%}
```{.python code-line-numbers="|1-3|5-8|10"}
encoder = Sequential()
for units in layer_sizes:
  encoder.add(Dense(units, activation="relu"))

decoder = Sequential()
for units in reversed(layer_sizes[:-1]):
  decoder.add(Dense(units, activation="relu"))
decoder.add(Dense(input_dim, activation="sigmoid"))

autoencoder = Sequential([encoder, decoder])
```

![](images/model.png){fig-align="center"}
:::

::::

::: {.notes}
- Adam
  - Adaptive Moment Estimation
  - Adaptive learning rate method
  - Computes individual adaptive learning rates for different parameters
    from estimates of first and second moments of the gradients

- ReduceLROnPlateau
  - Reduce learning rate when a metric has stopped improving
    - patience = 5
    - factor = 0.5
    - min_delta = 0.1
    - min_lr = 0.00001
:::

## Methods: Optimization {.smaller}

::: {.columns}

::: {.column width=50%}
- How does total *Number of Neurons* affect performance?
- How does *Number of Hidden Layers* affect performance?
  - Distributed the neurons so that each layer has ~half the neurons
    of the previous layer
- Used *Random Search* to find optimal hyperparameters
  - `n_neurons` = 1, 10, 100, 200, 400, 600, 800, 1000, 2000
  - `n_layers` = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10
:::

::: {.column .nonincremental width=50%}
![](images/ae.png){fig-align="center"}
:::

::::

# Results

## Results: Dimension Reduction {.smaller}

::: {.columns}

::: {.column width=50%}
![](images/pca.png){.fragment fig-align="center"}
:::

::: {.column width=50%}
![](images/tsne.png){.fragment fig-align="center"}
:::

::::

::: {.notes}
- D.R. on the 21953 genes remaining
  - Visualized using PCA and t-SNE
  - Labelled using the most common GO term - "intracellular anatomical structure"
  - Generally bad performance
  - Visualized others, performance not much better
- PCA
  - Doesn't resolve the clusters well
  - First 2 PCs only account for ~4.4% of variance
- t-SNE
  - Hoped it would be better
  - Doesn't resolve the clusters well
  - Need to spend more time fine-tuning the hyperparameters
:::

## Results: Optimization & Performance {.smaller}

::: {.columns}

::: {.column width=50%}
![](images/optimization-hidden-units.png){.fragment fig-align="center"}
:::

::: {.column width=50%}
![](images/optimization-hidden-layers.png){.fragment fig-align="center"}
:::

::: {.fragment}
::: {.callout}
## With Best Hyperparameters {.smaller}
Cross Validation Loss: $0.9632$
:::
:::

::::

::: {.notes}
- Used 1 hidden layer
  - Evaluated NUM_HIDDEM from 1 to 2000
  - Loss improves as the number of neurons increases - 1000, 2000 best
- Used 2000 HU and 1000 HU
  - Evaluated NUM_LAYER from 1 to 10
  - Best performance with 1 layer
  - Plateaus after 2 layers
  - 1000 HU gets worse with more
- Best performance with 1 layer and 2000 HU
  - Cross Validation Loss
:::

## Discussion {.smaller}

- `Limitations`
  - Was only able to use 21953/48978 genes due to memory constraints with GPU training
  - Could not evaluate more hyperparameters
  - Was not able to evaluate performance on GO term prediction

- `Future Work`
  - Improve the performance of the autoencoder; try different architectures
    such as sparse and variational autoencoders
  - Find a larger / more balanced dataset

# Thank You


## References {.smaller}

- [Gene Ontology](http://geneontology.org/)
- [Digital Expression Explorer 2](https://dee2.io/index.html)
- [Autoencoders](https://en.wikipedia.org/wiki/Autoencoder)
