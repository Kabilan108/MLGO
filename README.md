# **MLGO:** Machine Learning for Gene Ontology Terms

## Compression of Differential Expression Data with Deep Autoencoders


### Folder Structure

```
.
├── README.md                       [Project description and installation instructions]
├── config/config.yaml              [Configuration file for project]
├── docs
│   ├── images                      [Contains images used in the report and presentation]
│   ├── includes                    [Contains files used to generate the report]
│   ├── presentation.qmd            [Presentation in quarto format]
│   ├── presentation.pdf            [Presentation]
│   ├── report.pdf                  [Project report]
│   └── report.tex                  [Project report in LaTeX format]
├── index.yaml                      [Contains project details]
├── mlgo                            [Python package for project]
├── notebooks
│   ├── figures/                    [Contains figures generated by notebooks]
│   ├── project.ipynb               [Notebook used for analysis]
│   └── preliminaryanalysis.ipynb   [Notebook used for preliminary analysis]
└── scripts
    ├── cleandata.R                 [Prepares dataframes with GO terms and log fold changes]
    ├── loaddatasets.R              [Downloads datasets and performs DGE analysis]
    ├── loadmetadata.R              [Loads metadata for datasets]
    ├── prepdata.py                 [Prepares training, validation, and test data]
    └── process_batches.sh          [Runs all scripts to download and process data]
```

### Installation & Use

- All dependencies for this project are contained in `requirements.txt`. To
  install dependencies, run `pip3 install -r requirements.txt`
- To download the datasets, you can run `bash scripts/process_batches.sh`
  - This will download the datasets and perform the DGE analysis and GO
    enrichment. It will then prepare the data for use in the deep learning
    model.
  - Note that this will take a long time to run, and will require >20GB of
    storage space and >16GB of RAM.
  - Alternatively, you can download the data from my Dropbox, [here](https://www.dropbox.com/s/if2x86765uc5l1k/data.tar.gz\?dl\=1) or by using the commands below.
    - This will create a folder called data in the PATH_TO_DATA_FOLDER
      directory.
    - **Update the paths in the [config.yaml](config/config.yaml) file to point
      to the data folder.**
- Run the `notebooks/project.ipynb` notebook to reproduce the results in the
  report.
- **I strongly advise using a Colab notebook or something similar
  due to the size of the data files being used, and the especially for
  training the neural networks.** If you do not, you may run into memory
  issues with tensorflow and sklearn.

```bash
# Download preprocessed data from Dropbox
wget "https://www.dropbox.com/s/if2x86765uc5l1k/data.tar.gz\?dl\=1" -O data.tar.gz
tar -xvf data.tar.gz -C PATH_TO_DATA_FOLDER
```
