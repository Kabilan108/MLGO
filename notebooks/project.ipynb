{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression of Differential Expression Data with Deep Autoencoders\n",
    "\n",
    "**By [Tony Kabilan Okeke](mailto:tko35@drexel.edu)**\n",
    "\n",
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from numpy.random import shuffle\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import losses\n",
    "from keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from mlgo import utils\n",
    "\n",
    "with open(\"config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data\n",
    "\n",
    "The data used in this notebook was generated by running the `process_batches.sh`\n",
    "script in the `scripts` directory. The script downloads raw data from DEE2,\n",
    "and performs Differential Gene Expression analysis on each batch. The results\n",
    "are combined into a data matrix containing the log2 fold change of each gene\n",
    "and the enriched GO terms for each dataset processed. These were then split\n",
    "into training, validation, and test sets. See the README for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train, test and validation data\n",
    "with np.load(f\"{config['path']['final']}/labels.npz\", allow_pickle=True) as D:\n",
    "    labels = D['labels']\n",
    "    features = D['features']\n",
    "    samples = D['samples']\n",
    "\n",
    "with np.load(f\"{config['path']['final']}/train.npz\") as D:\n",
    "    X_train, Y_train = D['X'], D['Y']\n",
    "\n",
    "with np.load(f\"{config['path']['final']}/test.npz\") as D:\n",
    "    X_test, Y_test = D['X'], D['Y']\n",
    "\n",
    "with np.load(f\"{config['path']['final']}/val.npz\") as D:\n",
    "    X_val, Y_val = D['X'], D['Y']\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Combine data and drop low variance features\n",
    "X = np.concatenate([X_train, X_test, X_val])\n",
    "sel = VarianceThreshold(threshold=1)\n",
    "sel.fit(X)\n",
    "\n",
    "# Update training and validation data\n",
    "X_train = X[:len(X_train), sel.get_support()]\n",
    "X_test = X[:len(X_test), sel.get_support()]\n",
    "X_val = X[:len(X_val), sel.get_support()]\n",
    "\n",
    "# Combine data for visualization\n",
    "X = np.concatenate([X_train, X_test, X_val])\n",
    "Y = np.concatenate([Y_train, Y_test, Y_val])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Here, I will visualize the data using PCA with different GO terms serving as labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "fig, ax = utils.plotpca(X_pca, pca.explained_variance_ratio_, Y[:,0])\n",
    "ax.set_title(f\"PCA for {labels[0]}\")\n",
    "plt.savefig('notebooks/results/pca.png', dpi=300, transparent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "tsne = TSNE(n_components=2, n_jobs=mp.cpu_count(), perplexity=50)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "g = sns.scatterplot(\n",
    "    x=X_tsne[:, 0], y=X_tsne[:, 1], hue=Y[:,100], s=100, alpha=0.3, ax=ax\n",
    ")\n",
    "g.legend(loc=\"upper right\", fontsize=13, frameon=False)\n",
    "ax.set_xlabel(f\"t-SNE 1\", fontsize=13)\n",
    "ax.set_ylabel(f\"t-SNE 2\", fontsize=13)\n",
    "ax.set_title(f\"t-SNE for {labels[0]}\", fontsize=16)\n",
    "utils.format(ax)\n",
    "plt.savefig('notebooks/results/tsne.png', dpi=300, transparent=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Model\n",
    "\n",
    "Here, I will define the architecture for the autoencoder network I am evaluating.\n",
    "The function defined below will return a Keras model object with the specified\n",
    "number of hidden layers and nodes per layer.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "Here, we will evaluate the impact of varying the number of hidden layers and\n",
    "the total number of hidden units on the performance of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "INPUT_DIM = X_train.shape[1]\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "\n",
    "# Define callback for early stopping\n",
    "callbacks = [\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.5, patience=5, min_delta=0.1, min_lr=0.00001\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function will be used to train the model during each iteration of the\n",
    "hyperparameter optimization.\n",
    "\"\"\"\n",
    "def evaluate_model():\n",
    "    # Build the model\n",
    "    model = utils.build_autoencoder(NUM_LAYERS, NUM_HIDDEN, INPUT_DIM)\n",
    "    model.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        X_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_val, X_val),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Store the best validation loss\n",
    "    best_loss = history.history['loss'][-1]\n",
    "    best_val_loss = history.history['val_loss'][-1]\n",
    "    with open('notebooks/results/optimization.tsv', 'a') as f:\n",
    "        f.write(f'{NUM_LAYERS}\\t{NUM_HIDDEN}\\t{best_loss}\\t{best_val_loss}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying the Total Number of Hidden Units\n",
    "\n",
    "Here, we will evaluate the impact of varying the total number of hidden units\n",
    "with only one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the number of layers\n",
    "hidden = [1, 10, 100, 200, 400, 600, 800, 1000, 2000]\n",
    "\n",
    "# Keep number of layers constant\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "for NUM_HIDDEN in hidden:\n",
    "    # Train the model and store the results\n",
    "    p = mp.Process(target=evaluate_model)\n",
    "    p.start()\n",
    "    p.join()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "results = pd.read_csv('notebooks/results/optimization.tsv', sep='\\t')\n",
    "results = results[results['num_layers'] == 1]\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.plot(results['num_hidden'], results['loss'], label='Training')\n",
    "ax.plot(results['num_hidden'], results['val_loss'], label='Validation')\n",
    "ax.set_xlabel('Number of Hidden Units', fontsize=13)\n",
    "ax.set_ylabel('Loss (MSE)', fontsize=13)\n",
    "ax.legend(fontsize=13, fancybox=False)\n",
    "utils.format(ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebooks/results/optimization-hidden-units.png', dpi=300, transparent=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Varying the Number of Hidden Layers\n",
    "\n",
    "Based on the results from the previous experiment, the optimal number of hidden\n",
    "units is 2000. Here, we will evaluate the impact of varying the number of hidden\n",
    "layers with a total of 2000 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the number of layers\n",
    "layers = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# Keep number of hidden units constant\n",
    "hidden = [1000, 2000]\n",
    "\n",
    "for NUM_HIDDEN in hidden:\n",
    "    for NUM_LAYERS in layers:\n",
    "        # Train the model and store the results\n",
    "        p = mp.Process(target=evaluate_model)\n",
    "        p.start()\n",
    "        p.join()\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "results = pd.read_csv('notebooks/results/optimization.tsv', sep='\\t')\n",
    "results = results[results['num_hidden'] >= 1000]\n",
    "\n",
    "# Use Dark2 color palette\n",
    "colors = ['#FF0000', '#000000', '#E7298A', '#66A61E']\n",
    "\n",
    "# Plot the results (use solid lines for 2000 hidden units and dashed lines for 1000)\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "for i, (NUM_HIDDEN, ls) in enumerate(zip(hidden[::-1], ['--', '-'])):\n",
    "    sub = results[results['num_hidden'] == NUM_HIDDEN]\n",
    "    ax.plot(\n",
    "        sub['num_layers'], sub['loss'], linestyle=ls, color=colors[i],\n",
    "        linewidth=2,\n",
    "        label=f'Training ({NUM_HIDDEN} HU)'\n",
    "    )\n",
    "    ax.plot(\n",
    "        sub['num_layers'], sub['val_loss'], linestyle=ls, color=colors[i+2],\n",
    "        linewidth=2,\n",
    "        label=f'Validation ({NUM_HIDDEN} HU)'\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Number of Hidden Layers', fontsize=13)\n",
    "ax.set_ylabel('Loss (MSE)', fontsize=13)\n",
    "ax.legend(fontsize=13, loc='upper left')\n",
    "utils.format(ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig('notebooks/results/optimization-hidden-layers.png', dpi=300, transparent=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation with Cross-Validation\n",
    "\n",
    "Using the best combination of hyperparameters, we will evaluate the performance\n",
    "of the autoencoder using 4-fold cross-validation.\n",
    "\n",
    "**Optimal Hyperparameters:**\n",
    "- `NUM_HIDDEN`: 2000\n",
    "- `NUM_LAYERS`: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the results file\n",
    "with open('notebooks/results/crossval.tsv', 'w') as f:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function modified for Cross-Validation\n",
    "\"\"\"\n",
    "def evaluate_model():\n",
    "    # Build the model\n",
    "    model = utils.build_autoencoder(NUM_LAYERS, NUM_HIDDEN, INPUT_DIM)\n",
    "    model.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_fold_train, \n",
    "        X_fold_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_fold_val, X_fold_val),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Store the best validation loss\n",
    "    best_val_loss = history.history['val_loss'][-1]\n",
    "    with open('notebooks/results/crossval.tsv', 'a') as f:\n",
    "        f.write(f'{best_val_loss}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Best Model\n",
    "NUM_LAYERS = 1\n",
    "NUM_HIDDEN = 2000\n",
    "\n",
    "# Define folds for cross-validation\n",
    "NFOLDS = 4\n",
    "\n",
    "# Split training data\n",
    "shuffle(X_train)\n",
    "shuffle(X_val)\n",
    "training = np.array_split(X_train, NFOLDS)\n",
    "validation = np.array_split(X_val, NFOLDS)\n",
    "\n",
    "# Cross-validation\n",
    "val_loss = []\n",
    "for X_fold_train, X_fold_val in zip(training, validation):\n",
    "    # Train the model and store the results\n",
    "    p = mp.Process(target=evaluate_model)\n",
    "    p.start()\n",
    "    p.join()\n",
    "\n",
    "# Report the average validation loss\n",
    "results = pd.read_csv('notebooks/results/crossval.tsv', sep='\\t', header=None)\n",
    "print(f\"Cross-Validation Loss: {results.mean().values[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model architecture\n",
    "model = utils.build_autoencoder(NUM_LAYERS, NUM_HIDDEN, INPUT_DIM)\n",
    "model.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "plot_model(model, to_file='notebooks/results/model.png', show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37a863925245f63826c76e03d588b260b21a6d5417ede353df91ee186be6f027"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
